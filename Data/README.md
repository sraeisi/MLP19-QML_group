Quantum Machine Learning Project
====
Data is  Stored in a ".npy" Format which Contains 100000 Samples.
in order to successfully run the Code you need to put all of ".npy" Files in the Same Directory as the Code
or Generate your own Data (Which Will take a Longer Time to Run)



<!--
Data size was bigger than 25MB so it couldn't be uploaded on github, you can find the data on dropbox :

Data is a pandas DataFrame which should be imported via :

url = 'https://www.dropbox.com/s/zapb2jjm0ihp14a/data.gz?dl=1'

s=requests.get(url, stream=True).content

data=pd.read_pickle(io.BytesIO(s) , compression='gz')


DataFrame contains 500,000 records of random density matrices and contains following fields:

- 15 features for the density matrix, which is feature_i,j = tr(\rho * (sigma_i \ sigma_j)) where sigma_4 = I the identity operator,
and sigma_1 = sigma_x , sigma_2 = sigma_y , sigma_3 = sigma_z are the pauli matrices.

- the PPT criterion, which is the determinant of the partial transpose of the bi-partite system. The PPT criterion says that if this value is negative, then the density matrix is entangled and if positive, then it is seprable.

- the binary class of these matrices, where label '1' is for entangled matrices where '0' denotes the class of seprable matrices.
-->



#### data analysis

The random matrices are generated by qutip library with qutip.rand_dm method, and then the entanglement is calculated by determinant of the partial transposed matrix. Then the labels can be determined by the sign of this value.

There is an issue about the rand_dm method in qutip library, which it doesn't generate symmetric random matrices in each x,y and z direction. It could be easily found from the histogram of these 15 features, where distributions in correspondance to z axis are different from ones to x and y axes.

#### preparing data

Our data is cleaned up by itself, because it is synthetically generated, so there isn't any missing data and the class values are binary numeric values '0' and '1'.

In "code.ipynb" file, the data has been prepared in **Preparing Data** section. First, the data has been scaled by three different scalers. Then each scaled data has been given to different regressors and classifiers.

#### Feature selection
Important features can be determined by tree classifiers/regressors and the magnitute of linear classifiers/regressors coefs. In the **Feature Importance** section of "code.ipynb", these analyses could be found.


<!--

- Over Quite Some time that I ve run some Codes 
I think the **Decision Tree Regressor** can Provide a good Classification of Entangled States over Separable ones .
-->
 
#### Feature Extraction and feature reduction
In "code.ipynb" file there are sections related to **the data reduction** where feature extraction and feature reduction techniques such as PCA,RFE,... could be found.


----
The code is jupyter notebook using python3 kernel.

The required packages to run the code are:

numpy

scipy

pandas

matplotlib

qutip

scikit-learn

Cython
