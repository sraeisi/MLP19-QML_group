Quantum Machine Learning Project
====
Data is a pandas DataFrame which should be imported via :

data=pd.read_pickle('data.gz' , compression='gz')


DataFrame contains 10,000 records of random density matrices and contains following fields:

- 15 features for the density matrix, which 

$$Features = \begin{bmatrix}
\sigma_x\otimes\sigma_x & \sigma_x\otimes\sigma_y & \sigma_x\otimes\sigma_z & \sigma_x\otimes\ \mathbb{1} 
\\ \sigma_y\otimes\sigma_x & \sigma_y\otimes\sigma_y & \sigma_y\otimes\sigma_z & \sigma_y\otimes\ \mathbb{1}
\\ \sigma_z\otimes\sigma_x & \sigma_z\otimes\sigma_y & \sigma_z\otimes\sigma_z & \sigma_z\otimes\ \mathbb{1}
\\ \mathbb{1}\otimes\sigma_x & \mathbb{1}\otimes\sigma_y & \mathbb{1}\otimes\sigma_z & \mathbb{1}\otimes\ \mathbb{1}
\end{bmatrix}$$


sigma_x, sigma_y and sigma_z are the pauli matrices.

- the PPT criterion, which is the determinant of the partial transpose of the bi-partite system. The PPT criterion says that if this value is negative, then the density matrix is entangled and if positive, then it is seprable.

- the binary class of these matrices, where label '1' is for entangled matrices where '0' denotes the class of seprable matrices.



#### Data Analysis

The random matrices are generated by qutip library with qutip.rand_dm method, and then the entanglement is calculated by determinant of the partial transposed matrix. Then the labels can be determined by the sign of this value.

There is an issue about the rand_dm method in qutip library, which it doesn't generate symmetric random matrices in each x,y and z direction. It could be easily found from the histogram of these 15 features, where distributions in correspondance to z axis are different from ones to x and y axes.

All kinds of histograms and analysis could be found in **Data Analysis** section of "code.ipynb".

#### Preparing the Data

Our data is cleaned up by itself, because it is synthetically generated, so there isn't any missing data and the class values are binary numeric values '0' and '1'.

In "code.ipynb" file, the data has been prepared in **Preparing Data** section. First, the data has been scaled by three different scalers. Then each scaled data has been given to different regressors and classifiers.

#### Feature selection
Important features can be determined by tree classifiers/regressors and the magnitute of linear classifiers/regressors coefs. In the **Feature Importance** section of "code.ipynb", these analyses could be found.


<!--

- Over Quite Some time that I ve run some Codes 
I think the **Decision Tree Regressor** can Provide a good Classification of Entangled States over Separable ones .
-->
 
#### Feature Extraction and Feature Reduction
In "code.ipynb" file there are sections related to **Data reduction** where feature extraction and feature reduction techniques such as PCA,RFE,... could be found.


----
The code is jupyter notebook using python3 kernel.

The required packages to run the code are:

numpy

scipy

pandas

matplotlib

qutip

scikit-learn

Cython
